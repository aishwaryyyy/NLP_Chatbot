{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pratyush-Arora/NLP_CHATBOT/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yU75cwMkl9_",
        "outputId": "317dad22-4136-411d-b976-d2978ed1b9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, I am the Retrieval Learning Bot. Start typing your text after greeting to talk to me. For ending convo, type \"bye!\"\n",
            "hi\n",
            "Bot: hi\n",
            "tell me about pizzo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: [18]\n",
            "the lombardic word bizzo or pizzo meaning \"mouthful\" (related to the english words \"bit\" and \"bite\"), which was brought to italy in the middle of the 6th century ad by the invading lombards.\n",
            "bye\n",
            "Bot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Open the text file for reading\n",
        "with open('/content/pizza_wiki.txt', 'r', errors='ignore') as f:\n",
        "    raw_doc = f.read()\n",
        "\n",
        "# Convert the text to lowercase\n",
        "raw_doc = raw_doc.lower()\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Tokenize the text into sentences and words\n",
        "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
        "word_tokens = nltk.word_tokenize(raw_doc)\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# Define a function to lemmatize tokens\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Create a dictionary to remove punctuation\n",
        "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "# Define a function to lemmatize and normalize text\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n",
        "\n",
        "# Greeting inputs and responses\n",
        "greet_inputs = ('hello', 'hi', 'whassup', 'how are you?')\n",
        "greet_responses = ('hi', 'Hey', 'Hey There!', 'There there!')\n",
        "\n",
        "# Define a greeting function\n",
        "def greet(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in greet_inputs:\n",
        "            return random.choice(greet_responses)\n",
        "\n",
        "# Initialize a flag\n",
        "flag = True\n",
        "\n",
        "print('Hello, I am the Retrieval Learning Bot. Start typing your text after greeting to talk to me. For ending convo, type \"bye!\"')\n",
        "\n",
        "while flag:\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "\n",
        "    if user_response != 'bye':\n",
        "        if user_response == 'thank you' or user_response == 'thanks':\n",
        "            flag = False\n",
        "            print('Bot: You are Welcome')\n",
        "        else:\n",
        "            greeting = greet(user_response)\n",
        "            if greeting is not None:\n",
        "                print('Bot: ' + greeting)\n",
        "            else:\n",
        "                sentence_tokens.append(user_response)\n",
        "                word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "                final_words = list(set(word_tokens))\n",
        "\n",
        "                # Get the response using TF-IDF and cosine similarity\n",
        "                TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "                tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
        "                vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "                idx = vals.argsort()[0][-2]\n",
        "                flat = vals.flatten()\n",
        "                flat.sort()\n",
        "                req_tfidf = flat[-2]\n",
        "                if req_tfidf == 0:\n",
        "                    print(\"Bot: I am sorry. Unable to understand you!\")\n",
        "                else:\n",
        "                    robo1_response = sentence_tokens[idx]\n",
        "                    print('Bot: ' + robo1_response)\n",
        "                sentence_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag = False\n",
        "        print('Bot: Goodbye!')\n"
      ]
    }
  ]
}